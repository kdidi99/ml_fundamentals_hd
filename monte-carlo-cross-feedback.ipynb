{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "monte-carlo-cross-feedback.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyORQD/2oSVN1IpXx26Itqdm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MEFsyXy5a3a"
      },
      "source": [
        "# Cross-Feedback for Exercise 1a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhUeStzU6T01"
      },
      "source": [
        "Unfortunately, the solution has been passed to us without a .ipynb file. This is why we have to give our feedback in this seperate file. We tried to make it as clear as possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsSrxEd95mWz"
      },
      "source": [
        "## 1.1 Data Creation and Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrbfyXv05yOz"
      },
      "source": [
        "The group did a great job here. They first demonstrate the procedure of inverse tranform sampling via plotting the likelihoods, the CDFs and the inverse transforms. The code here is also commented which helps to understand the procedure. They also add graphs not necessary for the exercise but helpful for understanding. The lambda fct use can be hard to understand for people not familiar with it, but is used cleverly here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKX7S2ZH5zCJ"
      },
      "source": [
        "## 1.2 Classification by Thresholding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5Zm4rba55cB"
      },
      "source": [
        "The function to calculate the error rates for a given classifier works similar to the function given in the sample solution, except that it integrated the repetition over ten data samples and gives back not only the error rate but also the standard deviation of the rate. That makes the code more compact than the sample solution but gives away the possibility of calculating the error rate of an arbitrary sample of datasets (for example just one data set).\n",
        "\n",
        "The plotted standard deviations seem to be to small for higher sample sizes than 10 because the standard deviation should decrease only by a factor of about 3 for 10 times the sample size. But in this solution the standard deviation is not detectable for sample sizes above 10. I could not find the position of the corresponding error in the code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVrCmX3c56T6"
      },
      "source": [
        "## 1.3 Baseline Classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjORRh6i5_gy"
      },
      "source": [
        "You capsuled and reused your code (e.g. *test_threshold* function), which is good.\n",
        "\n",
        "Some comments would make your code easier to read.\n",
        "\n",
        "The results of your measurement seem to be correct. Good idea to include guessing and bayes error in the plot!\n",
        "Standard deviation is only visible for sample size of 10. But this might just be an issue of scaling (?).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_gA5KJU6AWr"
      },
      "source": [
        "## Nearest Neighbour Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lU1wQab86Mma"
      },
      "source": [
        "[22]: \n",
        "\n",
        "Correction: *… response vector **Y** of length…*\n",
        "\n",
        "The while True loop in *create_data_2* might not be the most performant way to implement this. Even though it will terminate at some point, it is not deterministic. Setting the first value in Y to 0 and the second value to 1 after calling randint is faster.\n",
        "\n",
        "Unlike the sample solution, you did not create a function that generates a fully balanced training set, which is totally fine, because it is not what the task asked you to do.\n",
        "\n",
        "\n",
        "[23]: \n",
        "\n",
        "I think the idea of the nearest neighbour classifier is to pass a training set and a single feature, of which the predicted label is returned. Passing a single feature instead of an array of features might be a bit more elegant and makes the function more simple. You could calculate the distance between *Xtest* and each *Xtrain* and then simply use argmin to determine the label to return.\n",
        "Of course, you would need another loop in [24] and [25] then.\n",
        "\n",
        "Nevertheless, your implementation seems to be working (can't try because jupyter notebook is missing). Results seem to be okay (0.391 for N = 2 might be a bit high though?). \n",
        "\n",
        "Good idea to calculate the standard deviation as well!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxg_Tu5J6NHn"
      },
      "source": [
        "## Appendix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cN26cvgG6Q4W"
      },
      "source": [
        "Good idea to try the nearest neighbour classifier with the 10 nearest neighbours! Very interesting result."
      ]
    }
  ]
}