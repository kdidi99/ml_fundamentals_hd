{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kdidi99/ml_heidelberg/blob/main/tree_methods_stubs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hF_WRksvtyv"
      },
      "source": [
        "# Preliminaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {
        "id": "zAsYoozYvty0"
      },
      "outputs": [],
      "source": [
        "# import modules\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.datasets import load_digits\n",
        "import matplotlib.pyplot as p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "7isFQGjzvty2"
      },
      "outputs": [],
      "source": [
        "# base classes\n",
        "\n",
        "class Node:\n",
        "    pass\n",
        "\n",
        "class Tree:\n",
        "    def __init__(self):\n",
        "        self.root = Node()\n",
        "    \n",
        "    def find_leaf(self, x):\n",
        "        node = self.root\n",
        "        while hasattr(node, \"feature\"):\n",
        "            j = node.feature\n",
        "            if x[j] <= node.threshold:\n",
        "                node = node.left\n",
        "            else:\n",
        "                node = node.right\n",
        "        return node"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVsse16Bvty4"
      },
      "source": [
        "# Density Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "UWEqJnm4vty6"
      },
      "outputs": [],
      "source": [
        "class DensityTree(Tree):\n",
        "    def __init__(self):\n",
        "        super(DensityTree, self).__init__()\n",
        "        \n",
        "    def train(self, data, prior, n_min=20):\n",
        "        '''\n",
        "        data: the feature matrix for the digit under consideration\n",
        "        prior: the prior probability of this digit\n",
        "        n_min: termination criterion (don't split if a node contains fewer instances)\n",
        "        '''\n",
        "        self.prior = prior\n",
        "        N, D = data.shape\n",
        "        D_try = int(np.sqrt(D)) # number of features to consider for each split decision\n",
        "\n",
        "        # find and remember the tree's bounding box, \n",
        "        # i.e. the lower and upper limits of the training feature set\n",
        "        m, M = np.min(data, axis=0), np.max(data, axis=0)\n",
        "        self.box = m.copy(), M.copy()\n",
        "        \n",
        "        # identify invalid features and adjust the bounding box\n",
        "        # (If m[j] == M[j] for some j, the bounding box has zero volume, \n",
        "        #  causing divide-by-zero errors later on. We must exclude these\n",
        "        #  features from splitting and adjust the bounding box limits \n",
        "        #  such that invalid features have no effect on the volume.)\n",
        "        valid_features   = np.where(m != M)[0]\n",
        "        invalid_features = np.where(m == M)[0]\n",
        "        M[invalid_features] = m[invalid_features] + 1\n",
        "\n",
        "        # initialize the root node\n",
        "        self.root.data = data\n",
        "        self.root.box = m.copy(), M.copy()\n",
        "\n",
        "        # build the tree\n",
        "        stack = [self.root]\n",
        "        while len(stack):\n",
        "            node = stack.pop()\n",
        "            n = node.data.shape[0] # number of instances in present node\n",
        "            if n >= n_min:\n",
        "                # Call 'make_density_split_node()' with 'D_try' randomly selected \n",
        "                # indices from 'valid_features'. This turns 'node' into a split node\n",
        "                # and returns the two children, which must be placed on the 'stack'.\n",
        "                permutation = np.random.permutation(len(valid_features))[:D_try] #+\n",
        "                feature_indices = valid_features[permutation] #+\n",
        "                left, right = make_density_split_node(node, N, feature_indices) #+\n",
        "                stack.append(left) #+\n",
        "                stack.append(right) #+\n",
        "            else:\n",
        "                # Call 'make_density_leaf_node()' to turn 'node' into a leaf node.\n",
        "                make_density_leaf_node(node, N) #+ ### ab \"if\"-Zeile fast gleicher code auch im Decision tree, wenn \n",
        "                                                    # Fehler auftritt muss wahrscheinlich beides korrigiert werden\n",
        "\n",
        "    def predict(self, x):\n",
        "        leaf = self.find_leaf(x)\n",
        "        # return p(x | y) * p(y) if x is within the tree's bounding box \n",
        "        # and return 0 otherwise\n",
        "        if (x >= leaf.box[0]).all() and (x <= leaf.box[1]).all(): #+\n",
        "            return leaf.response*self.prior #+\n",
        "        else: #+\n",
        "            return 0 #+"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "zwt-_HGHvty8"
      },
      "outputs": [],
      "source": [
        "def make_density_split_node(node, N, feature_indices):\n",
        "    '''\n",
        "    node: the node to be split\n",
        "    N:    the total number of training instances for the current class\n",
        "    feature_indices: a numpy array of length 'D_try', containing the feature \n",
        "                     indices to be considered in the present split\n",
        "    '''\n",
        "    n, D = node.data.shape\n",
        "    m, M = node.box\n",
        "\n",
        "    # find best feature j (among 'feature_indices') and best threshold t for the split\n",
        "    e_min = float(\"inf\")\n",
        "    j_min, t_min = None, None\n",
        "    \n",
        "    for j in feature_indices:\n",
        "        # Hint: For each feature considered, first remove duplicate feature values using \n",
        "        # 'np.unique()'. Describe here why this is necessary:\n",
        "        # This is necessary because the thresholds are placed in the middle of the gaps between\n",
        "        # the values after sorting them. There is no gap between two equal values which is why\n",
        "        # duplicates need to be removed. A threshold must not be equal to a feature, because \n",
        "        # it would not be possible to decide whether the feature goes to the left or the right node \n",
        "        data_unique = np.unique(node.data[:, j]) #+\n",
        "        # Compute candidate thresholds\n",
        "        data_unique = np.sort(data_unique) #+\n",
        "        #tj = [(data_unique[i]+data_unique[i+1])/2 for i in range(len(data_unique)-1)] #+\n",
        "        tj = (data_unique[:len(data_unique)-1] + np.roll(data_unique, -1)[:len(data_unique)-1]) / 2 #+\n",
        "        \n",
        "        # Illustration: for loop - hint: vectorized version is possible\n",
        "        for t in tj:\n",
        "            # compute the error for left child when splitting j at t \n",
        "            data_l = node.data[node.data[:,j] <= t] #+\n",
        "            n_l = len(data_l) #+\n",
        "            box_l = np.max(data_l, axis=0) - np.min(data_l, axis=0) #+\n",
        "            box_l[box_l == 0] = 1 #+\n",
        "            v_l = np.product(box_l) #+\n",
        "            loo_err_l = (n_l / (N * v_l)) * ((n_l / N) - 2 * ((n_l - 1)/(N - 1))) #+\n",
        "\n",
        "            # compute the error for right child when splitting j at t \n",
        "            data_r = node.data[node.data[:,j] > t] #+\n",
        "            n_r = len(data_r) #+\n",
        "            box_r = np.max(data_r, axis=0) - np.min(data_r, axis=0) #+\n",
        "            box_r[box_r == 0] = 1 #+\n",
        "            v_r = np.product(box_r) #+\n",
        "            loo_err_r = (n_r / (N * v_r)) * ((n_r / N) - 2 * ((n_r - 1)/(N - 1))) #+\n",
        "\n",
        "            # compute overall error (we want to minimize the error for both \n",
        "            # the left and the right child, so we minimize the sum)\n",
        "            loo_error = loo_err_l + loo_err_r #+\n",
        "            \n",
        "            # choose the best threshold that\n",
        "            if loo_error < e_min:\n",
        "                e_min = loo_error #+\n",
        "                j_min = j #+\n",
        "                t_min = t #+\n",
        "\n",
        "    # create children\n",
        "    left = Node()\n",
        "    right = Node()\n",
        "\n",
        "    # initialize 'left' and 'right' with the data subsets and bounding boxes\n",
        "    # according to the optimal split found above\n",
        "    #left.data = [i for i in node.data if i[j_min]<t_min] #+ # store data in left node -- for subsequent splits\n",
        "    left.data = node.data[node.data[:,j_min] < t_min] #+\n",
        "    left.box = np.min(left.data, axis=0).copy(), np.max(left.data, axis=0).copy() #+ # store bounding box in left node\n",
        "    #right.data = [i for i in node.data if i[j_min]>t_min] #+\n",
        "    right.data = node.data[node.data[:,j_min] > t_min] #+\n",
        "    right.box = np.min(right.data, axis=0).copy(), np.max(right.data, axis=0).copy() #+\n",
        "\n",
        "    # turn the current 'node' into a split node\n",
        "    # (store children and split condition)\n",
        "    node.left = left\n",
        "    node.right = right\n",
        "    node.feature = j_min #+\n",
        "    node.threshold = t_min #+\n",
        "\n",
        "    # return the children (to be placed on the stack)\n",
        "    return left, right"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "id": "2jSRCsVbvty9"
      },
      "outputs": [],
      "source": [
        "def make_density_leaf_node(node, N):\n",
        "    '''\n",
        "    node: the node to become a leaf\n",
        "    N:    the total number of training instances for the current class\n",
        "    '''\n",
        "    # compute and store leaf response\n",
        "    n = node.data.shape[0] #+\n",
        "    box = node.box[1]-node.box[0]\n",
        "    box[box == 0] = 1\n",
        "    v = np.product(box) #+\n",
        "    node.response = n/(N*v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQLKTED9vty-"
      },
      "source": [
        "# Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "7T1OkbXzvty_"
      },
      "outputs": [],
      "source": [
        "class DecisionTree(Tree):\n",
        "    def __init__(self):\n",
        "        super(DecisionTree, self).__init__()\n",
        "        \n",
        "    def train(self, data, labels, n_min=20):\n",
        "        '''\n",
        "        data: the feature matrix for all digits\n",
        "        labels: the corresponding ground-truth responses\n",
        "        n_min: termination criterion (don't split if a node contains fewer instances)\n",
        "        '''\n",
        "        N, D = data.shape\n",
        "        D_try = int(np.sqrt(D)) # how many features to consider for each split decision\n",
        "\n",
        "        # initialize the root node\n",
        "        self.root.data = data\n",
        "        self.root.labels = labels\n",
        "        \n",
        "        stack = [self.root]\n",
        "        while len(stack):\n",
        "            node = stack.pop()\n",
        "            n = node.data.shape[0] # number of instances in present node\n",
        "            if n >= n_min and not node_is_pure(node):\n",
        "                # Call 'make_decision_split_node()' with 'D_try' randomly selected \n",
        "                # feature indices. This turns 'node' into a split node\n",
        "                # and returns the two children, which must be placed on the 'stack'.\n",
        "                feature_indices = np.random.permutation(D)[:D_try] #+c\n",
        "                left, right = make_decision_split_node(node, feature_indices) #+c+\n",
        "                stack.append(left) #+c\n",
        "                stack.append(right) #+c\n",
        "            else:\n",
        "                # Call 'make_decision_leaf_node()' to turn 'node' into a leaf node.\n",
        "                make_decision_leaf_node(node) #+c+ ### ab \"if\"-Zeile praktisch kopiert vom Density tree, wenn\n",
        "                                                    # Fehler auftritt muss wahrscheinlich beides korrigiert werden\n",
        "                \n",
        "    def predict(self, x):\n",
        "        leaf = self.find_leaf(x)\n",
        "        # compute p(y | x)\n",
        "        index = np.argmax(leaf.response, axis=0) #+\n",
        "        y = leaf.response[index[0],1].astype(int)\n",
        "        return y #+"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "iMBq5BaZvtzA"
      },
      "outputs": [],
      "source": [
        "def make_decision_split_node(node, feature_indices):\n",
        "    '''\n",
        "    node: the node to be split\n",
        "    feature_indices: a numpy array of length 'D_try', containing the feature \n",
        "                     indices to be considered in the present split\n",
        "    '''\n",
        "    n, D = node.data.shape\n",
        "\n",
        "    # find best feature j (among 'feature_indices') and best threshold t for the split\n",
        "    gini_min = float(\"inf\") #+c\n",
        "    j_min, t_min = None, None #+c\n",
        "\n",
        "    for j in feature_indices: #+c\n",
        "        data_unique = np.unique(node.data[:, j]) #+c\n",
        "        # Compute candidate thresholds\n",
        "        data_unique = np.sort(data_unique) #+c\n",
        "        tj = (data_unique[:len(data_unique)-1] + np.roll(data_unique, -1)[:len(data_unique)-1]) / 2 #+c\n",
        "\n",
        "        for t in tj: #+c\n",
        "            # compute the gini impurity for left child when splitting j at t \n",
        "            data_l = node.data[node.data[:,j] <= t] #+c\n",
        "            labels_l = node.labels[node.data[:,j] <= t] #+\n",
        "            ulabels_l = np.unique(labels_l) #+\n",
        "            n_l = len(data_l) #+c\n",
        "            n_lk = np.zeros(len(ulabels_l)) #+\n",
        "\n",
        "            for k in range(len(ulabels_l)): #+\n",
        "                n_lk[k] = np.sum(labels_l == ulabels_l[k]) #+\n",
        "            \n",
        "            gini_l = n_l * (1 - np.sum(np.square(n_lk) / (n_l ** 2))) #+\n",
        "\n",
        "            # compute the gini impurity for right child when splitting j at t \n",
        "            data_r = node.data[node.data[:,j] > t] #+c\n",
        "            labels_r = node.labels[node.data[:,j] > t] #+\n",
        "            ulabels_r = np.unique(labels_r) #+\n",
        "            n_r = len(data_r) #+c\n",
        "            n_rk = np.zeros(len(ulabels_r)) #+\n",
        "\n",
        "            for k in range(len(ulabels_r)): #+\n",
        "                n_rk[k] = np.sum(labels_r == ulabels_r[k]) #+\n",
        "            \n",
        "            gini_r = n_r * (1 - np.sum(np.square(n_rk) / (n_r ** 2))) #+\n",
        "            \n",
        "            # compute overall gini impurity (we want to minimize it for both \n",
        "            # the left and the right child, so we minimize the sum)\n",
        "            gini = gini_l + gini_r #+\n",
        "\n",
        "            if gini < gini_min: #+c\n",
        "                gini_min = gini #+c\n",
        "                j_min = j #+c\n",
        "                t_min = t #+c\n",
        "\n",
        "    # create children\n",
        "    left = Node()\n",
        "    right = Node()\n",
        "    \n",
        "    # initialize 'left' and 'right' with the data subsets and labels\n",
        "    # according to the optimal split found above\n",
        "    left.data = node.data[node.data[:,j_min] < t_min] #+c\n",
        "    left.labels = node.labels[node.data[:,j_min] < t_min] #+\n",
        "    #left.labels = [node.labels[i] for i in range(len(n)) if node.data[i][j_min]<t_min] #+ # corresponding labels\n",
        "    right.data = node.data[node.data[:,j_min] > t_min] #+c\n",
        "    right.labels = node.labels[node.data[:,j_min] > t_min] #+\n",
        "    #right.labels = [node.labels[i] for i in range(len(n)) if node.data[i][j_min]>t_min] #+\n",
        "\n",
        "    # turn the current 'node' into a split node\n",
        "    # (store children and split condition)\n",
        "    node.left = left\n",
        "    node.right = right\n",
        "    node.feature = j_min #+c\n",
        "    node.threshold = t_min #+c    \n",
        "\n",
        "    # return the children (to be placed on the stack)\n",
        "    return left, right    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "0fzB5njuvtzB"
      },
      "outputs": [],
      "source": [
        "def make_decision_leaf_node(node):\n",
        "    '''\n",
        "    node: the node to become a leaf\n",
        "    '''\n",
        "    # compute and store leaf response\n",
        "    node.N = len(node.labels) #+\n",
        "\n",
        "    unique = np.unique(node.labels)\n",
        "    node.response = np.empty((len(unique),2)) #+ liste mit elementen [p(label), label]\n",
        "\n",
        "    for i in range(len(unique)): #+\n",
        "        node.response[i, 0] = np.sum(node.labels == unique[i])/node.N #+\n",
        "        node.response[i, 1] = unique[i] #+\n",
        "    \n",
        "    #node.response = np.array([np.array([np.sum(node.labels == i)/node.N, i]) for i in np.unique(node.labels)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "g--qzuK9vtzB"
      },
      "outputs": [],
      "source": [
        "def node_is_pure(node):\n",
        "    '''\n",
        "    check if 'node' ontains only instances of the same digit\n",
        "    '''\n",
        "    return len(np.unique(node.labels)) == 1 #+"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXTCuDW1vtzC"
      },
      "source": [
        "# Evaluation of Density and Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "t2YcwpZavtzD"
      },
      "outputs": [],
      "source": [
        "# read and prepare the digits data\n",
        "digits = load_digits() \n",
        "data = digits[\"data\"]\n",
        "images = digits[\"images\"]\n",
        "target = digits[\"target\"]\n",
        "target_names = digits[\"target_names\"]\n",
        "\n",
        "X_all = data\n",
        "y_all = target\n",
        "\n",
        "y_unique = np.unique(target)\n",
        "X_group = np.zeros(len(y_unique), dtype=object)\n",
        "y_group = np.zeros(len(y_unique), dtype=object)\n",
        "\n",
        "for i in range(len(y_unique)):\n",
        "    X_group[i] = data[target == y_unique[i]]\n",
        "    y_group[i] = target[target == y_unique[i]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "cIFnJpNnvtzD"
      },
      "outputs": [],
      "source": [
        "# train trees, plot training error confusion matrices, and comment on your results\n",
        "\n",
        "# train density trees\n",
        "den_trees = np.empty(len(y_unique), dtype=object)\n",
        "\n",
        "for i in range(len(y_unique)):\n",
        "    den_trees[i] = DensityTree()\n",
        "    den_trees[i].train(X_group[i], 1/10, 20)\n",
        "\n",
        "# train decision tree\n",
        "dec_tree = DecisionTree()\n",
        "dec_tree.train(X_all, y_all, 20)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test both trees\n",
        "\n",
        "def den_trees_predict(x):\n",
        "    predictions = np.empty(len(y_unique))\n",
        "    for i in range(len(y_unique)):\n",
        "        predictions[i] = den_trees[i].predict(x)\n",
        "    return y_unique[np.argmax(predictions)]\n",
        "\n",
        "print(f\"True: {y_all[13]}, Predict: {den_trees_predict(X_all[13])}\")\n",
        "print(f\"True: {y_all[13]}, Predict: {dec_tree.predict(X_all[13])}\")\n",
        "\n",
        "den_tree_result = np.empty(len(y_all))\n",
        "dec_tree_result = np.empty(len(y_all))\n",
        "\n",
        "for i in range(len(X_all)):\n",
        "    den_tree_result[i] = den_trees_predict(X_all[i])  \n",
        "    dec_tree_result[i] = dec_tree.predict(X_all[i])   \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "du-4t1DJzSXd",
        "outputId": "16cd0e94-f143-481d-a19e-dbec3abd5106"
      },
      "execution_count": 233,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True: 3, Predict: 3\n",
            "True: 3, Predict: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# useful function to compute and print out the confusion matrix\n",
        "def confusion_matrix(predicted, true, print_result = True):\n",
        "    # all possible classes in training and testing data\n",
        "    classes = np.unique(np.concatenate([true, predicted]))\n",
        "\n",
        "    # for each matrix entry in row c_t, column c_p, count\n",
        "    # for how many examples is the true label c_t and the predicted label c_p.\n",
        "    matrix = [[np.sum((true==c_t) * (predicted==c_p)) / np.sum(true==c_t) for c_t in classes] for c_p in classes]\n",
        "\n",
        "    if print_result:\n",
        "        print('Class | ' +' |'.join(['%5d'%(c) for c in classes]))\n",
        "        for c, row in zip(classes, matrix):\n",
        "            print('------+-' + '+'.join(['------' for c in classes]))\n",
        "            print('%5d | '%(c) + '|'.join(['%5.1f%%'%(100.*r) for r in row]))\n",
        "        print()\n",
        "\n",
        "    return matrix\n",
        "\n",
        "print(\"Training error confusion matrix for Density Tree:\\n\")\n",
        "mat = confusion_matrix(den_tree_result, y_all)\n",
        "print(\"\\nTraining error confusion matrix for Decision Tree:\\n\")\n",
        "mat = confusion_matrix(dec_tree_result, y_all)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMzVgY-Kzw9G",
        "outputId": "6f41eeb8-df96-4464-8689-42226534ae39"
      },
      "execution_count": 237,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training error confusion matrix for Density Tree:\n",
            "\n",
            "Class |     0 |    1 |    2 |    3 |    4 |    5 |    6 |    7 |    8 |    9\n",
            "------+-------+------+------+------+------+------+------+------+------+------\n",
            "    0 | 100.0%|  0.0%|  0.0%|  0.0%|  0.0%|  0.0%|  0.0%|  0.0%|  0.0%|  0.0%\n",
            "------+-------+------+------+------+------+------+------+------+------+------\n",
            "    1 |   0.0%|100.0%|  0.0%|  0.0%|  0.0%|  0.0%|  0.0%|  0.0%|  0.0%|  0.0%\n",
            "------+-------+------+------+------+------+------+------+------+------+------\n",
            "    2 |   0.0%|  0.0%|100.0%|  0.0%|  0.0%|  0.0%|  0.0%|  0.0%|  0.0%|  0.0%\n",
            "------+-------+------+------+------+------+------+------+------+------+------\n",
            "    3 |   0.0%|  0.0%|  0.0%|100.0%|  0.0%|  0.0%|  0.0%|  0.0%|  0.0%|  0.0%\n",
            "------+-------+------+------+------+------+------+------+------+------+------\n",
            "    4 |   0.0%|  0.0%|  0.0%|  0.0%|100.0%|  0.0%|  0.0%|  0.0%|  0.0%|  0.0%\n",
            "------+-------+------+------+------+------+------+------+------+------+------\n",
            "    5 |   0.0%|  0.0%|  0.0%|  0.0%|  0.0%|100.0%|  0.0%|  0.0%|  0.0%|  0.0%\n",
            "------+-------+------+------+------+------+------+------+------+------+------\n",
            "    6 |   0.0%|  0.0%|  0.0%|  0.0%|  0.0%|  0.0%|100.0%|  0.0%|  0.0%|  0.0%\n",
            "------+-------+------+------+------+------+------+------+------+------+------\n",
            "    7 |   0.0%|  0.0%|  0.0%|  0.0%|  0.0%|  0.0%|  0.0%|100.0%|  0.0%|  0.0%\n",
            "------+-------+------+------+------+------+------+------+------+------+------\n",
            "    8 |   0.0%|  0.0%|  0.0%|  0.0%|  0.0%|  0.0%|  0.0%|  0.0%|100.0%|  0.0%\n",
            "------+-------+------+------+------+------+------+------+------+------+------\n",
            "    9 |   0.0%|  0.0%|  0.0%|  0.0%|  0.0%|  0.0%|  0.0%|  0.0%|  0.0%|100.0%\n",
            "\n",
            "\n",
            "Training error confusion matrix for Decision Tree:\n",
            "\n",
            "Class |     0 |    1 |    2 |    3 |    4 |    5 |    6 |    7 |    8 |    9\n",
            "------+-------+------+------+------+------+------+------+------+------+------\n",
            "    0 |  97.2%|  0.0%|  1.7%|  0.5%|  0.6%|  2.7%|  1.7%|  2.2%|  2.3%|  3.3%\n",
            "------+-------+------+------+------+------+------+------+------+------+------\n",
            "    1 |   0.0%| 82.4%|  5.6%|  3.8%|  1.7%|  3.3%|  2.8%|  1.7%|  8.0%|  2.2%\n",
            "------+-------+------+------+------+------+------+------+------+------+------\n",
            "    2 |   0.0%|  1.6%| 80.8%|  2.7%|  0.6%|  1.1%|  0.6%|  1.1%|  2.9%|  1.7%\n",
            "------+-------+------+------+------+------+------+------+------+------+------\n",
            "    3 |   0.0%|  0.5%|  2.8%| 82.0%|  0.0%|  1.1%|  0.0%|  2.2%|  0.6%|  6.1%\n",
            "------+-------+------+------+------+------+------+------+------+------+------\n",
            "    4 |   1.7%|  0.0%|  0.0%|  0.0%| 90.6%|  0.0%|  0.0%|  1.7%|  1.1%|  1.7%\n",
            "------+-------+------+------+------+------+------+------+------+------+------\n",
            "    5 |   0.0%|  4.4%|  0.0%|  3.8%|  0.6%| 85.7%|  1.1%|  2.8%|  4.0%|  2.8%\n",
            "------+-------+------+------+------+------+------+------+------+------+------\n",
            "    6 |   0.0%|  1.1%|  0.6%|  0.5%|  1.7%|  1.6%| 90.6%|  0.0%|  0.6%|  0.6%\n",
            "------+-------+------+------+------+------+------+------+------+------+------\n",
            "    7 |   0.0%|  1.1%|  1.7%|  1.1%|  2.8%|  1.1%|  1.7%| 85.5%|  2.9%|  7.2%\n",
            "------+-------+------+------+------+------+------+------+------+------+------\n",
            "    8 |   0.0%|  7.1%|  3.4%|  2.7%|  1.7%|  1.1%|  1.1%|  1.1%| 73.0%|  0.0%\n",
            "------+-------+------+------+------+------+------+------+------+------+------\n",
            "    9 |   1.1%|  1.6%|  3.4%|  2.7%|  0.0%|  2.2%|  0.6%|  1.7%|  4.6%| 74.4%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNSWFX7nvtzE"
      },
      "source": [
        "# Density and Decision Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "jcasjB9tvtzE"
      },
      "outputs": [],
      "source": [
        "class DensityForest():\n",
        "    def __init__(self, n_trees):\n",
        "        # create ensemble\n",
        "        self.trees = [DensityTree() for i in range(n_trees)]\n",
        "    \n",
        "    def train(self, data, prior, n_min=20):\n",
        "        for tree in self.trees:\n",
        "            # train each tree, using a bootstrap sample of the data\n",
        "            ... # your code here\n",
        "\n",
        "    def predict(self, x):\n",
        "        # compute the ensemble prediction\n",
        "        return ... # your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "KbzcYuWfvtzG"
      },
      "outputs": [],
      "source": [
        "class DecisionForest():\n",
        "    def __init__(self, n_trees):\n",
        "        # create ensemble\n",
        "        self.trees = [DecisionTree() for i in range(n_trees)]\n",
        "    \n",
        "    def train(self, data, labels, n_min=0):\n",
        "        for tree in self.trees:\n",
        "            # train each tree, using a bootstrap sample of the data\n",
        "            ... # your code here\n",
        "\n",
        "    def predict(self, x):\n",
        "        # compute the ensemble prediction\n",
        "        return ... # your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiVGFHiNvtzG"
      },
      "source": [
        "# Evaluation of Density and Decision Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "2UbnkcssvtzH"
      },
      "outputs": [],
      "source": [
        "# train forests (with 20 trees per forest), plot training error confusion matrices, and comment on your results\n",
        "... # your code here"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "name": "tree-methods-stubs.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}